{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9caa00b4",
   "metadata": {},
   "source": [
    "# Classification of Sugarcane Diseases based on Images\n",
    "\n",
    "## Initial Setup\n",
    "\n",
    "Examining the train data shows that there are six (6) classes in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd64df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"Banded_Chlorosis\",\n",
    "    \"Brown_Rust\",\n",
    "    \"Brown_Spot\",\n",
    "    \"Viral\",\n",
    "    \"Yellow_Leaf\",\n",
    "    \"Healthy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5418fd",
   "metadata": {},
   "source": [
    "To make it easier to handle the image data, a Python class called `Classification` is defined and it will use the OpenCV library.\n",
    "\n",
    "This class will contain the method `load_images` that can be used to load the images, as well as the method `resize_images` for resizing all the images.\n",
    "\n",
    "Property methods `image_count` and `image_dimensions` also allow us to analyze the loaded images and determine if further preprocessing is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fdc1e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Classification:\n",
    "    name: str\n",
    "    images: list[np.ndarray]\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.images = list()\n",
    "\n",
    "    @property\n",
    "    def image_count(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @property\n",
    "    def image_dimensions(self) -> dict[tuple[int, int, int], int]:\n",
    "        dims = {}\n",
    "        for img in self.images:\n",
    "            if dims.get(img.shape) is None:\n",
    "                dims[img.shape] = 1\n",
    "                continue\n",
    "            dims[img.shape] += 1\n",
    "        return dims\n",
    "\n",
    "    @property\n",
    "    def image_dimensions_distribution(self) -> dict[tuple[int, int, int], int]:\n",
    "        distrib = {}\n",
    "        for key, value in self.image_dimensions.items():\n",
    "            distrib[key] = value / self.image_count\n",
    "        return distrib\n",
    "\n",
    "    def load_images(self, top_folder: str = \"train\") -> None:\n",
    "        \"\"\"\n",
    "        Empties self.images  then loads images using opencv imread\n",
    "\n",
    "        Returns the length of image_array\n",
    "\n",
    "        Raises exception upon error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.images = list()\n",
    "            path = Path(f\"{top_folder}/{self.name}\")\n",
    "            image_paths = [img_path for img_path in path.iterdir() if img_path.is_file()]\n",
    "            for path in image_paths:\n",
    "                self.images.append(cv2.imread(str(path)))\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def square_images(self, image_size: int = 512, with_padding: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Resizes all images in self.image_array to the specified image size.\n",
    "\n",
    "        In the event that the source image is not a square and with_padding is True,\n",
    "        padding will be added to the smaller side to ensure aspect ratio of 1.0.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        for index, item in enumerate(self.images):\n",
    "            height, width, _ = self.images[index].shape\n",
    "            aspect_ratio = height / width\n",
    "\n",
    "            if not with_padding or aspect_ratio == 1:\n",
    "                self.images[index] = cv2.resize(item, (image_size, image_size))\n",
    "                continue\n",
    "\n",
    "            padding_color = (0, 0, 0)\n",
    "            if aspect_ratio > 1:\n",
    "                width_padding = (height - width) // 2\n",
    "                image = cv2.copyMakeBorder(\n",
    "                    item,\n",
    "                    0,\n",
    "                    0,\n",
    "                    width_padding,\n",
    "                    width_padding,\n",
    "                    cv2.BORDER_CONSTANT,\n",
    "                    value=padding_color,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                height_padding = (width - height) // 2\n",
    "                image = cv2.copyMakeBorder(\n",
    "                    item,\n",
    "                    height_padding,\n",
    "                    height_padding,\n",
    "                    0,\n",
    "                    0,\n",
    "                    cv2.BORDER_CONSTANT,\n",
    "                    value=padding_color,\n",
    "                )\n",
    "\n",
    "            self.images[index] = cv2.resize(image, (image_size, image_size))\n",
    "\n",
    "    def normalize_images(self) -> None:\n",
    "        \"\"\"\n",
    "        Normalizes the values to be from 0 to 1 instead of 0 to 255.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        for index, item in enumerate(self.images):\n",
    "            self.images[index] = item / 255.0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.name}\\n image count: {self.image_count}\\n image dimensions: {self.image_dimensions}\\n dimension distribution: {self.image_dimensions_distribution}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6186a70",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "First, create the instances for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bdc5813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [Classification(_class) for _class in classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c2b32",
   "metadata": {},
   "source": [
    "Then, for each instance, let us load the images under the path `./train/<class_name>`.\n",
    "\n",
    "We will also display the number of images as well as the dimensions of all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0845df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banded_Chlorosis\n",
      " image count: 424\n",
      " image dimensions: {(1024, 768, 3): 404, (576, 768, 3): 20}\n",
      " dimension distribution: {(1024, 768, 3): 0.9528301886792453, (576, 768, 3): 0.04716981132075472}\n",
      "Brown_Rust\n",
      " image count: 282\n",
      " image dimensions: {(1024, 768, 3): 280, (576, 768, 3): 2}\n",
      " dimension distribution: {(1024, 768, 3): 0.9929078014184397, (576, 768, 3): 0.0070921985815602835}\n",
      "Brown_Spot\n",
      " image count: 1550\n",
      " image dimensions: {(1024, 768, 3): 1481, (576, 768, 3): 69}\n",
      " dimension distribution: {(1024, 768, 3): 0.9554838709677419, (576, 768, 3): 0.044516129032258066}\n",
      "Viral\n",
      " image count: 597\n",
      " image dimensions: {(1024, 768, 3): 501, (576, 768, 3): 96}\n",
      " dimension distribution: {(1024, 768, 3): 0.8391959798994975, (576, 768, 3): 0.16080402010050251}\n",
      "Yellow_Leaf\n",
      " image count: 1074\n",
      " image dimensions: {(1024, 768, 3): 1005, (576, 768, 3): 69}\n",
      " dimension distribution: {(1024, 768, 3): 0.9357541899441341, (576, 768, 3): 0.06424581005586592}\n",
      "Healthy\n",
      " image count: 387\n",
      " image dimensions: {(1024, 768, 3): 374, (576, 768, 3): 13}\n",
      " dimension distribution: {(1024, 768, 3): 0.9664082687338501, (576, 768, 3): 0.03359173126614987}\n"
     ]
    }
   ],
   "source": [
    "for x in classifications:\n",
    "    x.load_images()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfe97b",
   "metadata": {},
   "source": [
    "Two important pieces of information can be gleaned from the output.\n",
    "\n",
    "1. It can be seen that the six (6) different classes have an imbalance in image count.\n",
    "\n",
    "This is a problem because it may introduce excessive bias in our models.\n",
    "To avoid this problem, when training the models, we must use proper sampling techniques to ensure equal class distribution.\n",
    "\n",
    "2. The images are rectangular and the dimensions are not homogenous.\n",
    "\n",
    "Most images have a size of `1024x768` but there are some whose size are `576x768` instead.\n",
    "To ensure that our model will be able to process these images later, we can pad the images to make them a square, and then we resize it.\n",
    "\n",
    "Lastly, let us normalize the data so that the values are from 0 to 1 instead of 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16970e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banded_Chlorosis\n",
      " image count: 424\n",
      " image dimensions: {(512, 512, 3): 424}\n",
      " dimension distribution: {(512, 512, 3): 1.0}\n",
      "Brown_Rust\n",
      " image count: 282\n",
      " image dimensions: {(512, 512, 3): 282}\n",
      " dimension distribution: {(512, 512, 3): 1.0}\n",
      "Brown_Spot\n",
      " image count: 1550\n",
      " image dimensions: {(512, 512, 3): 1550}\n",
      " dimension distribution: {(512, 512, 3): 1.0}\n",
      "Viral\n",
      " image count: 597\n",
      " image dimensions: {(512, 512, 3): 597}\n",
      " dimension distribution: {(512, 512, 3): 1.0}\n",
      "Yellow_Leaf\n",
      " image count: 1074\n",
      " image dimensions: {(512, 512, 3): 1074}\n",
      " dimension distribution: {(512, 512, 3): 1.0}\n",
      "Healthy\n",
      " image count: 387\n",
      " image dimensions: {(512, 512, 3): 387}\n",
      " dimension distribution: {(512, 512, 3): 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Resize all images to 512x512\n",
    "IMAGE_SIZE = 128\n",
    "for x in classifications:\n",
    "    x.square_images(128, with_padding=True)\n",
    "    x.normalize_images()\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a74b5",
   "metadata": {},
   "source": [
    "With the images resized, we must also transform our categorical features into a numeric array.\n",
    "\n",
    "This can be done using Scikit-learn's LabelBinarizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6302b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer classes: ['Banded_Chlorosis' 'Brown_Rust' 'Brown_Spot' 'Healthy' 'Viral'\n",
      " 'Yellow_Leaf']\n",
      "{'Banded_Chlorosis': array([1, 0, 0, 0, 0, 0]), 'Brown_Rust': array([0, 1, 0, 0, 0, 0]), 'Brown_Spot': array([0, 0, 1, 0, 0, 0]), 'Viral': array([0, 0, 0, 0, 1, 0]), 'Yellow_Leaf': array([0, 0, 0, 0, 0, 1]), 'Healthy': array([0, 0, 0, 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "binarizer = LabelBinarizer()\n",
    "class_labels: list[np.array] = binarizer.fit_transform(classes)  # type: ignore\n",
    "label_pairings: dict[str, np.ndarray] = dict(zip(classes, class_labels, strict=False))\n",
    "# Print output and classes to verify\n",
    "print(f\"Binarizer classes: {binarizer.classes_}\")\n",
    "print(label_pairings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411640c4",
   "metadata": {},
   "source": [
    "With our processed images and our transformed class labels, let use generate the arrays which will serve as our input array `X` and label array `Y`.\n",
    "\n",
    "This can be accomplished by combining the images of our classes and generating the corresponding label array using `label_pairings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "18845d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4314, 128, 128, 3)\n",
      "Y.shape:(4314, 6)\n"
     ]
    }
   ],
   "source": [
    "# Create our array X, the input array\n",
    "X: np.ndarray = np.vstack(\n",
    "    [np.array(class_.images) for class_ in classifications]\n",
    ")  # convert the image list of each class into an array then combine them\n",
    "\n",
    "# Create our array Y, the label array\n",
    "Y: np.ndarray = np.vstack(\n",
    "    [\n",
    "        [label_pairings[class_.name] for _ in range(class_.image_count)]\n",
    "        for class_ in classifications\n",
    "    ],\n",
    ")  # generate the list of one-hot encoded vectors for each class then combine them\n",
    "\n",
    "print(f\"X.shape: {X.shape}\\nY.shape:{Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84219a9",
   "metadata": {},
   "source": [
    "\n",
    "However, we must not forget that our classes are imbalanced.\n",
    "\n",
    "It is important that balance this out to avoid overfitting towards the classes with a much larger count.\n",
    "\n",
    "To balance our classes, we can used the Imbalanced-Learn library and then undersample the Majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f0916e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_balanced.shape: (1692, 128, 128, 3)\n",
      "Y_balanced.shape:(1692, 6)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "RANDOM_STATE_INT = 1738  # Seed for consistency\n",
    "sampler = RandomUnderSampler(random_state=RANDOM_STATE_INT, replacement=False)\n",
    "\n",
    "X_flattened = X.reshape(X.shape[0], -1)  # Flatten X for compatibility with RandomUnderSampler\n",
    "\n",
    "X_balanced: np.ndarray  # type annotation\n",
    "Y_balanced: np.ndarray  # type annotation\n",
    "X_balanced, Y_balanced = sampler.fit_resample(X_flattened, Y)  # type: ignore\n",
    "\n",
    "X_balanced = X_balanced.reshape(-1, *X.shape[1:])\n",
    "print(f\"X_balanced.shape: {X_balanced.shape}\\nY_balanced.shape:{Y_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794517fa",
   "metadata": {},
   "source": [
    "`X_balanced` and `Y_balanced` represent our data with balanced samples.\n",
    "\n",
    "This can be verified by the fact that the number of elements `1692` divided by our number of classes `6` results in `282`, which is the cardinality of our minority class, `Brown_Rust` according to our previous cells.\n",
    "\n",
    "Lastly, for validation purposes of our model, a train-test split will be conducted on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1936a4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      " X_train: 1353\n",
      " Y_train: 1353\n",
      " X_test: 339\n",
      " Y_test: 339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_balanced, Y_balanced, train_size=0.8, random_state=RANDOM_STATE_INT, shuffle=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shapes\\n X_train: {len(X_train)}\\n Y_train: {len(Y_train)}\\n X_test: {len(X_test)}\\n Y_test: {len(Y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b4929",
   "metadata": {},
   "source": [
    "## Method 1: Convolution Neural Network (CNN)\n",
    "\n",
    "The first method for solving this classification problem is through the use of CNN.\n",
    "\n",
    "We will be using the Keras and TensorFlow libraries.\n",
    "\n",
    "First, let us setup the environment and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c5d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "\n",
    "# Set Keras backend to use TensorFlow\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Define the expected inputs\n",
    "image_inputs = keras.Input(\n",
    "    shape=classifications[0].images[0].shape, dtype=classifications[0].images[0].dtype\n",
    ")\n",
    "\n",
    "# Define model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

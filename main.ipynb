{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9caa00b4",
   "metadata": {},
   "source": [
    "# Classification of Sugarcane Diseases based on Images\n",
    "\n",
    "## Initial Setup\n",
    "\n",
    "Examining the train data shows that there are six (6) classes in total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd64df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    \"Banded_Chlorosis\",\n",
    "    \"Brown_Rust\",\n",
    "    \"Brown_Spot\",\n",
    "    \"Viral\",\n",
    "    \"Yellow_Leaf\",\n",
    "    \"Healthy\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5418fd",
   "metadata": {},
   "source": [
    "To make it easier to handle the image data, a Python class called `Classification` is defined and it will use the OpenCV library.\n",
    "\n",
    "This class will contain the method `load_images` that can be used to load the images, as well as the method `resize_images` for resizing all the images.\n",
    "\n",
    "Property methods `image_count` and `image_dimensions` also allow us to analyze the loaded images and determine if further preprocessing is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdc1e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Classification:\n",
    "    name: str\n",
    "    images: list[np.ndarray]\n",
    "\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.images = list()\n",
    "\n",
    "    @property\n",
    "    def image_count(self) -> int:\n",
    "        return len(self.images)\n",
    "\n",
    "    @property\n",
    "    def image_dimensions(self) -> dict[tuple[int, int, int], int]:\n",
    "        dims = {}\n",
    "        for img in self.images:\n",
    "            if dims.get(img.shape) is None:\n",
    "                dims[img.shape] = 1\n",
    "                continue\n",
    "            dims[img.shape] += 1\n",
    "        return dims\n",
    "\n",
    "    @property\n",
    "    def image_dimensions_distribution(self) -> dict[tuple[int, int, int], int]:\n",
    "        distrib = {}\n",
    "        for key, value in self.image_dimensions.items():\n",
    "            distrib[key] = value / self.image_count\n",
    "        return distrib\n",
    "\n",
    "    def load_images(self, top_folder: str = \"train\") -> None:\n",
    "        \"\"\"\n",
    "        Empties self.images  then loads images using opencv imread\n",
    "\n",
    "        Returns the length of image_array\n",
    "\n",
    "        Raises exception upon error\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.images = list()\n",
    "            path = Path(f\"{top_folder}/{self.name}\")\n",
    "            image_paths = [img_path for img_path in path.iterdir() if img_path.is_file()]\n",
    "            for path in image_paths:\n",
    "                self.images.append(cv2.imread(str(path)))\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def square_images(self, image_size: int = 512, with_padding: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Resizes all images in self.image_array to the specified image size.\n",
    "\n",
    "        In the event that the source image is not a square and with_padding is True,\n",
    "        padding will be added to the smaller side to ensure aspect ratio of 1.0.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        for index, item in enumerate(self.images):\n",
    "            height, width, _ = self.images[index].shape\n",
    "            aspect_ratio = height / width\n",
    "\n",
    "            if not with_padding or aspect_ratio == 1:\n",
    "                self.images[index] = cv2.resize(item, (image_size, image_size))\n",
    "                continue\n",
    "\n",
    "            padding_color = (0, 0, 0)\n",
    "            if aspect_ratio > 1:\n",
    "                width_padding = (height - width) // 2\n",
    "                image = cv2.copyMakeBorder(\n",
    "                    item,\n",
    "                    0,\n",
    "                    0,\n",
    "                    width_padding,\n",
    "                    width_padding,\n",
    "                    cv2.BORDER_CONSTANT,\n",
    "                    value=padding_color,\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                height_padding = (width - height) // 2\n",
    "                image = cv2.copyMakeBorder(\n",
    "                    item,\n",
    "                    height_padding,\n",
    "                    height_padding,\n",
    "                    0,\n",
    "                    0,\n",
    "                    cv2.BORDER_CONSTANT,\n",
    "                    value=padding_color,\n",
    "                )\n",
    "\n",
    "            self.images[index] = cv2.resize(image, (image_size, image_size))\n",
    "\n",
    "    def normalize_images(self) -> None:\n",
    "        \"\"\"\n",
    "        Normalizes the values to be from 0 to 1 instead of 0 to 255.\n",
    "\n",
    "        Returns None\n",
    "        \"\"\"\n",
    "        for index, item in enumerate(self.images):\n",
    "            self.images[index] = item / 255.0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.name}\\n image count: {self.image_count}\\n image dimensions: {self.image_dimensions}\\n dimension distribution: {self.image_dimensions_distribution}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6186a70",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "First, create the instances for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdc5813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [Classification(_class) for _class in classes]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c2b32",
   "metadata": {},
   "source": [
    "Then, for each instance, let us load the images under the path `./train/<class_name>`.\n",
    "\n",
    "We will also display the number of images as well as the dimensions of all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0845df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banded_Chlorosis\n",
      " image count: 424\n",
      " image dimensions: {(1024, 768, 3): 404, (576, 768, 3): 20}\n",
      " dimension distribution: {(1024, 768, 3): 0.9528301886792453, (576, 768, 3): 0.04716981132075472}\n",
      "Brown_Rust\n",
      " image count: 282\n",
      " image dimensions: {(1024, 768, 3): 280, (576, 768, 3): 2}\n",
      " dimension distribution: {(1024, 768, 3): 0.9929078014184397, (576, 768, 3): 0.0070921985815602835}\n",
      "Brown_Spot\n",
      " image count: 1550\n",
      " image dimensions: {(1024, 768, 3): 1481, (576, 768, 3): 69}\n",
      " dimension distribution: {(1024, 768, 3): 0.9554838709677419, (576, 768, 3): 0.044516129032258066}\n",
      "Viral\n",
      " image count: 597\n",
      " image dimensions: {(1024, 768, 3): 501, (576, 768, 3): 96}\n",
      " dimension distribution: {(1024, 768, 3): 0.8391959798994975, (576, 768, 3): 0.16080402010050251}\n",
      "Yellow_Leaf\n",
      " image count: 1074\n",
      " image dimensions: {(1024, 768, 3): 1005, (576, 768, 3): 69}\n",
      " dimension distribution: {(1024, 768, 3): 0.9357541899441341, (576, 768, 3): 0.06424581005586592}\n",
      "Healthy\n",
      " image count: 387\n",
      " image dimensions: {(1024, 768, 3): 374, (576, 768, 3): 13}\n",
      " dimension distribution: {(1024, 768, 3): 0.9664082687338501, (576, 768, 3): 0.03359173126614987}\n"
     ]
    }
   ],
   "source": [
    "for x in classifications:\n",
    "    x.load_images()\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfe97b",
   "metadata": {},
   "source": [
    "Two important pieces of information can be gleaned from the output.\n",
    "\n",
    "1. It can be seen that the six (6) different classes have an imbalance in image count.\n",
    "\n",
    "This is a problem because it may introduce excessive bias in our models.\n",
    "To avoid this problem, when training the models, we must use proper sampling techniques to ensure equal class distribution.\n",
    "\n",
    "2. The images are rectangular and the dimensions are not homogenous.\n",
    "\n",
    "Most images have a size of `1024x768` but there are some whose size are `576x768` instead.\n",
    "To ensure that our model will be able to process these images later, we can pad the images to make them a square, and then we resize it.\n",
    "\n",
    "Lastly, let us normalize the data so that the values are from 0 to 1 instead of 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16970e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banded_Chlorosis\n",
      " image count: 424\n",
      " image dimensions: {(128, 128, 3): 424}\n",
      " dimension distribution: {(128, 128, 3): 1.0}\n",
      "Brown_Rust\n",
      " image count: 282\n",
      " image dimensions: {(128, 128, 3): 282}\n",
      " dimension distribution: {(128, 128, 3): 1.0}\n",
      "Brown_Spot\n",
      " image count: 1550\n",
      " image dimensions: {(128, 128, 3): 1550}\n",
      " dimension distribution: {(128, 128, 3): 1.0}\n",
      "Viral\n",
      " image count: 597\n",
      " image dimensions: {(128, 128, 3): 597}\n",
      " dimension distribution: {(128, 128, 3): 1.0}\n",
      "Yellow_Leaf\n",
      " image count: 1074\n",
      " image dimensions: {(128, 128, 3): 1074}\n",
      " dimension distribution: {(128, 128, 3): 1.0}\n",
      "Healthy\n",
      " image count: 387\n",
      " image dimensions: {(128, 128, 3): 387}\n",
      " dimension distribution: {(128, 128, 3): 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Resize all images to 512x512\n",
    "IMAGE_SIZE = 128\n",
    "for x in classifications:\n",
    "    x.square_images(128, with_padding=True)\n",
    "    x.normalize_images()\n",
    "    pprint(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a74b5",
   "metadata": {},
   "source": [
    "With the images resized, we must also transform our categorical features into a numeric array.\n",
    "\n",
    "This can be done using Scikit-learn's LabelBinarizer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binarizer classes:\n",
      "array(['Banded_Chlorosis', 'Brown_Rust', 'Brown_Spot', 'Healthy', 'Viral',\n",
      "       'Yellow_Leaf'], dtype='<U16')\n",
      "\n",
      "{'Banded_Chlorosis': array([1, 0, 0, 0, 0, 0]),\n",
      " 'Brown_Rust': array([0, 1, 0, 0, 0, 0]),\n",
      " 'Brown_Spot': array([0, 0, 1, 0, 0, 0]),\n",
      " 'Viral': array([0, 0, 0, 0, 1, 0]),\n",
      " 'Yellow_Leaf': array([0, 0, 0, 0, 0, 1]),\n",
      " 'Healthy': array([0, 0, 0, 1, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "binarizer = LabelBinarizer()\n",
    "class_labels: list[np.array] = binarizer.fit_transform(classes)  # type: ignore\n",
    "label_pairings: dict[str, np.ndarray] = dict(zip(classes, class_labels, strict=False))\n",
    "# Print output and classes to verify\n",
    "print(\"Binarizer classes:\")\n",
    "pprint(binarizer.classes_)\n",
    "print()\n",
    "pprint(label_pairings, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411640c4",
   "metadata": {},
   "source": [
    "With our processed images and our transformed class labels, let use generate the arrays which will serve as our input array `X` and label array `Y`.\n",
    "\n",
    "This can be accomplished by combining the images of our classes and generating the corresponding label array using `label_pairings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18845d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4314, 128, 128, 3)\n",
      "Y.shape: (4314, 6)\n"
     ]
    }
   ],
   "source": [
    "# Create our array X, the input array\n",
    "X: np.ndarray = np.vstack(\n",
    "    [np.array(class_.images) for class_ in classifications]\n",
    ")  # convert the image list of each class into an array then combine them\n",
    "\n",
    "# Create our array Y, the label array\n",
    "Y: np.ndarray = np.vstack(\n",
    "    [\n",
    "        [label_pairings[class_.name] for _ in range(class_.image_count)]\n",
    "        for class_ in classifications\n",
    "    ],\n",
    ")  # generate the list of one-hot encoded vectors for each class then combine them\n",
    "\n",
    "print(f\"X.shape: {X.shape}\\nY.shape: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84219a9",
   "metadata": {},
   "source": [
    "\n",
    "However, we must not forget that our classes are imbalanced.\n",
    "\n",
    "It is important that balance this out to avoid overfitting towards the classes with a much larger count.\n",
    "\n",
    "To balance our classes, we can used the Imbalanced-Learn library and then undersample the Majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0916e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_balanced.shape: (1692, 128, 128, 3)\n",
      "Y_balanced.shape: (1692, 6)\n",
      "\n",
      "Class Distribution:\n",
      "{'[1 0 0 0 0 0]': 282,\n",
      " '[0 1 0 0 0 0]': 282,\n",
      " '[0 0 1 0 0 0]': 282,\n",
      " '[0 0 0 1 0 0]': 282,\n",
      " '[0 0 0 0 1 0]': 282,\n",
      " '[0 0 0 0 0 1]': 282}\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "RANDOM_STATE_INT = 1738  # Seed for consistency\n",
    "sampler = RandomUnderSampler(random_state=RANDOM_STATE_INT, replacement=False)\n",
    "\n",
    "X_flattened = X.reshape(X.shape[0], -1)  # Flatten X for compatibility with RandomUnderSampler\n",
    "\n",
    "X_balanced: np.ndarray  # type annotation\n",
    "Y_balanced: np.ndarray  # type annotation\n",
    "X_balanced, Y_balanced = sampler.fit_resample(X_flattened, Y)  # type: ignore\n",
    "\n",
    "X_balanced = X_balanced.reshape(-1, *X.shape[1:])\n",
    "print(f\"X_balanced.shape: {X_balanced.shape}\\nY_balanced.shape: {Y_balanced.shape}\")\n",
    "\n",
    "# Count the number of occurences per class\n",
    "class_distribution: dict[str, int] = {}  #\n",
    "for label in Y_balanced:\n",
    "    if class_distribution.get(f\"{label}\") is None:\n",
    "        class_distribution[f\"{label}\"] = 1\n",
    "        continue\n",
    "    class_distribution[f\"{label}\"] += 1\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "pprint(class_distribution, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794517fa",
   "metadata": {},
   "source": [
    "`X_balanced` and `Y_balanced` represent our data with balanced samples. Printing the class distribution also shows that each class has 282 entries each. This matches the cardinality of our minority class, `Brown_Rust`, as identified by our previous cells\n",
    "\n",
    "Lastly, for validation purposes of our model, a train-test split will be conducted on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1936a4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      " X_train: 1353\n",
      " Y_train: 1353\n",
      " X_test: 339\n",
      " Y_test: 339\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_balanced, Y_balanced, train_size=0.8, random_state=RANDOM_STATE_INT, shuffle=True\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Shapes\\n X_train: {len(X_train)}\\n Y_train: {len(Y_train)}\\n X_test: {len(X_test)}\\n Y_test: {len(Y_test)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b4929",
   "metadata": {},
   "source": [
    "## Method 1: Convolution Neural Network (CNN)\n",
    "\n",
    "The first method for solving this classification problem is through the use of CNN.\n",
    "\n",
    "We will be using the Keras and TensorFlow libraries.\n",
    "\n",
    "First, let us setup the environment and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c5d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 2.9418 - val_accuracy: 0.0000e+00 - val_loss: 1.7844\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 1.7722 - val_accuracy: 0.0000e+00 - val_loss: 1.6763\n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 1.6948 - val_accuracy: 0.0000e+00 - val_loss: 1.6939\n",
      "Epoch 4/10\n",
      "\u001b[1m2/6\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 1.6755"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m EPOCHS = \u001b[32m10\u001b[39m\n\u001b[32m     63\u001b[39m BATCH_SIZE = \u001b[32m250\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m history = \u001b[43mcnn_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\u001b[39;49;00m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Documents\\School\\UPD\\3RD YEAR\\2ND SEM 2024-2025\\CS180\\Project\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "\n",
    "# Set Keras backend to use TensorFlow\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "# Define our Model\n",
    "class CNN(keras.Model):\n",
    "    # Define the layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Convolution Layer 1\n",
    "        self.conv1 = keras.layers.Conv2D(\n",
    "            filters=64, kernel_size=(3, 3), padding=\"Same\", activation=\"relu\"\n",
    "        )\n",
    "        self.maxpool1 = keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        self.dropout1 = keras.layers.Dropout(0.25)\n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = keras.layers.Conv2D(\n",
    "            filters=128, kernel_size=(3, 3), padding=\"Same\", activation=\"relu\"\n",
    "        )\n",
    "        self.maxpool2 = keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        self.dropout2 = keras.layers.Dropout(0.25)\n",
    "        # Convolution Layer 3\n",
    "        self.conv3 = keras.layers.Conv2D(\n",
    "            filters=256, kernel_size=(3, 3), padding=\"Same\", activation=\"relu\"\n",
    "        )\n",
    "        self.maxpool3 = keras.layers.MaxPool2D(pool_size=(2, 2))\n",
    "        self.dropout3 = keras.layers.Dropout(0.25)\n",
    "        # fully connected\n",
    "        self.flatten = keras.layers.Flatten()\n",
    "        self.dense1 = keras.layers.Dense(256, activation=\"relu\")\n",
    "        self.dropout = keras.layers.Dropout(0.5)\n",
    "        self.output_layer = keras.layers.Dense(len(classes), activation=\"softmax\")\n",
    "        ...\n",
    "\n",
    "    # Define the forward pass\n",
    "    def call(self, inputs):\n",
    "        step1 = self.conv1(inputs)\n",
    "        step2 = self.maxpool1(step1)\n",
    "        step3 = self.dropout1(step2)\n",
    "        step4 = self.conv2(step3)\n",
    "        step5 = self.maxpool2(step4)\n",
    "        step6 = self.dropout2(step5)\n",
    "        step7 = self.conv3(step6)\n",
    "        step8 = self.maxpool3(step7)\n",
    "        step9 = self.dropout3(step8)\n",
    "        step10 = self.flatten(step9)\n",
    "        step11 = self.dense1(step10)\n",
    "        step12 = self.dropout(step11)\n",
    "        return self.output_layer(step12)\n",
    "\n",
    "\n",
    "cnn_model = CNN()\n",
    "cnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999),  # type: ignore\n",
    "    loss=keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=[keras.metrics.Accuracy()],\n",
    ")\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 250\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    # steps_per_epoch=X_train.shape[0] // BATCH_SIZE,\n",
    "    validation_data=(X_test, Y_test),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
